## Mixture of Experts - Pytorch

A Pytorch implementation of <a href="https://arxiv.org/abs/2006.16668">Mixture of Experts</a>, for enhancing attention networks. It will pretty much be a line-by-line transcription of the tensorflow implementation <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/moe.py">here</a>.

## Citation

```bibtex
@misc{lepikhin2020gshard,
    title 	= {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author 	= {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    year 	= {2020},
    eprint 	= {2006.16668},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}
```
